{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125dcf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['douglas adams']) {'tokens': ['davor', 'lauc'], 'labels': ('fn1_hr_Latn_HR', 'ln_hr_Latn_HR'), 'counts': 1041.249228961059, 'prob': 93.48} {'tokens': ['반', '기문'], 'labels': ('ln_ko_Hang_KR', 'fn1_ko_Hang_KR'), 'counts': 2159.896804479325, 'prob': 94.46}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from wiki_labels import qid_lab_get, qid_en_desc_get\n",
    "from wiki_location import q2cc\n",
    "from text_utils import cl\n",
    "import sys\n",
    "sys.path.insert(0, '/projekti/mondoAPI')\n",
    "from pnu.parse import parse\n",
    "from api.db import db\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(qid_lab_get(42, 'en').keys(), parse('davor lauc')['tags'][0], parse('반기문')['tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki lang to iso\n",
    "WIKI_DIR = '/backup/wikidata'\n",
    "api_key = open('airtable_key.txt').read()\n",
    "from pyairtable import Table, Api\n",
    "import sys\n",
    "sys.path.insert(0, '/projekti/nelma')\n",
    "from mondoDB.referencedb import provi\n",
    "\n",
    "if True:\n",
    "    lang2cc = defaultdict(Counter)\n",
    "    for k, v in provi.items():\n",
    "        lang, s, c = v['id'].split('_')\n",
    "        bod = float(v['Percent'])*3 if 'Percent' in v else 0\n",
    "        if 'Status' in v:\n",
    "            if v['Status'] == 'official':\n",
    "                bod += 1\n",
    "            elif v['Status'] == 'official_regional':\n",
    "                bod += 0.5\n",
    "            elif v['Status'] == 'de_facto_official':\n",
    "                bod += 0.9\n",
    "            elif v['Status'] == 'romanized':\n",
    "                bod += 0.2\n",
    "            elif v['Status'] == 'foreign':\n",
    "                bod += -0.5\n",
    "        if bod>1.1:\n",
    "            lang2cc[f'{lang}'][c] = max(bod, lang2cc[f'{lang}'][c])\n",
    "    api = Api(api_key)\n",
    "    w2iso = {t['fields']['WMF']: t['fields']['qid'] if 'qid' in t['fields'] else None for t in Table(api_key, 'appUZvAm9EHZgC1Eg', 'wiki_lang').all()}\n",
    "    wiki2cc = defaultdict(Counter)\n",
    "    for k, v in w2iso.items():\n",
    "        if len(k) == 2:\n",
    "            wiki2cc[v].update(lang2cc[k])\n",
    "        elif k.count('-') == 1:\n",
    "            a, b = k.split('-')\n",
    "            if len(a) == 2 and len(b) == 2:\n",
    "                wiki2cc[v][b.upper()] += 5 \n",
    "            elif len(a) == 2 and len(b) == 4:\n",
    "                #wiki2cc[v].update(lang2cc[f'{k}_{b.uppser()}'])\n",
    "                wiki2cc[v].update(lang2cc[a])\n",
    "    wikil2cc = {k: {cc: round(100*v2/max(v.values())) for cc, v2 in v.items()} for k, v in wiki2cc.items() if len(v)>0 and max(v.values())>0}\n",
    "\n",
    "cc2lang = defaultdict(set)\n",
    "for lang, ccs in lang2cc.items():\n",
    "    for cc in ccs:\n",
    "        if ccs[cc]>1:\n",
    "            cc2lang[cc].add(lang)\n",
    "iso2w = {k:v for v, k in w2iso.items()}\n",
    "\n",
    "wikil2cc['Q1860']['US'], cc2lang['CH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf37331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_cc(country, birth_death_place, language):\n",
    "    ccs = Counter()\n",
    "    for bp in birth_death_place:\n",
    "        if bp[5:] in q2cc:\n",
    "            ccs[q2cc[bp[5:]]] += 1\n",
    "    for cq in j['country']:\n",
    "        if cq[5:] in q2cc:\n",
    "            ccs[q2cc[cq[5:]]] += 2\n",
    "    for lng in language:\n",
    "        lng = lng[5:]\n",
    "        if lng in wikil2cc:\n",
    "            for cl,bod in wikil2cc[lng].items():\n",
    "                ccs[cl] += bod*2 / 100\n",
    "    cc,_ = ccs.most_common()[0] if len(ccs)>0 else ('', '')\n",
    "    return cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open('/backup/wikidata/wiki_person.tsv', 'w')\n",
    "fo.write('qid\\tname\\tfn\\tln\\tgender\\tdesc\\tplace\\tdob\\timage\\tsort\\n')\n",
    "for i, l in tqdm(enumerate(open('/backup/wikidata/wikinelma.jsonl')), total=17_630_406):\n",
    "    j = json.loads(l)\n",
    "    if j['type'] == 'per':\n",
    "        qid = int(j['wiki_id'][1:])\n",
    "        cc = get_wiki_cc(j['country']+j['nationality'], j['residence']+j['birth_place']+j['death_place'], j['native_language'])\n",
    "        name = None\n",
    "        if j['name_native']:\n",
    "            name = cl(j['name_native'][0]).lower()\n",
    "        else:\n",
    "            if j['native_language']:\n",
    "                langs = Counter([iso2w[q[5:]] for q in j['native_language'] if q[5:] in iso2w])\n",
    "            else:\n",
    "                langs = Counter()\n",
    "            langs.update(cc2lang[cc])\n",
    "            for l, f in langs.most_common():\n",
    "                name = qid_lab_get(qid, lang=l)\n",
    "                if name:\n",
    "                    name = list(name)[0]\n",
    "                    break\n",
    "        if not name:\n",
    "            try:\n",
    "                name = list(qid_lab_get(qid, 'en', include_alt=False).keys())[0]\n",
    "            except:\n",
    "                try:\n",
    "                    name = list(j['l'].values())[0][0]\n",
    "                except:\n",
    "                    name = ''\n",
    "        if name:\n",
    "            namel = name.lower().strip()\n",
    "            desc = qid_en_desc_get(qid)\n",
    "            dob = j['dob'][0][:4] if j['dob'] else ''\n",
    "            image = j['picture'][0] if j['picture'] else ''\n",
    "            sort = (int(dob) if dob else 1980)-(50 if image else 0)-(10 if desc else 0)\n",
    "\n",
    "            fn = ''\n",
    "            if j['fn']:\n",
    "                fns = qid_lab_get(int(j['fn'][0][6:]), include_alt=True).keys()\n",
    "                fns = Counter({a:len(a) for a in fns if a in namel})\n",
    "                if len(fns)>0:\n",
    "                    fn = fns.most_common()[0][0]\n",
    "\n",
    "            ln = ''\n",
    "            if j['ln']:\n",
    "                fns = qid_lab_get(int(j['ln'][0][6:]), include_alt=True).keys()\n",
    "                fns = Counter({a:len(a) for a in fns if a in namel})\n",
    "                if len(fns)>0:\n",
    "                    ln = fns.most_common()[0][0]\n",
    "            # only \n",
    "            gender = ''\n",
    "            if j['gender']==['WIKI_Q6581097']:\n",
    "                gender = 'm'\n",
    "            elif j['gender']==['WIKI_Q6581072']:\n",
    "                gender = 'f'\n",
    "            \n",
    "            if False and not fn or not ln:\n",
    "                if ',' in namel:\n",
    "                    if ' ' in namel.split(',')[0].strip():\n",
    "                        namel = namel.split(',')[0].strip()\n",
    "                    else:\n",
    "                        namel = namel.split(',')[1].strip()+' '+namel.split(',')[0].strip()\n",
    "                    #print(name, namel)\n",
    "                parts = namel.split(' ')\n",
    "                if len(parts) == 1 and fn:\n",
    "                    if namel.startswith(fn):\n",
    "                        parts = [fn, namel[len(fn):]]\n",
    "                    elif namel.endswith(fn):\n",
    "                        parts = [fn, namel[:len(namel)-len(fn)]]\n",
    "                if len(parts) == 1 and ln:\n",
    "                    if namel.startswith(ln):\n",
    "                        parts = [namel[len(ln):], ln]\n",
    "                    elif namel.endswith(ln):\n",
    "                        parts = [namel[:len(namel)-len(ln)], ln]\n",
    "\n",
    "                if len(parts) == 2:\n",
    "                    if fn and parts[0]==fn:\n",
    "                        ln = parts[1]\n",
    "                    elif fn and parts[1]==fn:\n",
    "                        ln = parts[0]\n",
    "                    elif ln and parts[1]==ln:\n",
    "                        fn = parts[0]\n",
    "                    elif ln and parts[0]==ln:\n",
    "                        fn = parts[1]\n",
    "                    else:\n",
    "                        fn, ln = parts\n",
    "                elif len(parts) > 2:\n",
    "                    fn, ln = parts[0], parts[-1]\n",
    "                    \n",
    "            if not cc:\n",
    "                cc = ''\n",
    "\n",
    "            fo.write(f'{qid}\\t{name}\\t{fn}\\t{ln}\\t{gender}\\t{desc}\\t{cc}\\t{dob}\\t{image}\\t{sort}\\n')\n",
    "fo.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77a599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6153032448fb4671b9bb8cbbabe54c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9928121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fo = open('/backup/wikidata/wiki_parsed.tsv', 'w')\n",
    "fi = open('/backup/wikidata/wiki_person.tsv', 'r')\n",
    "fi.readline()\n",
    "fo.write(f\"qid\\tname\\tprob\\ttitle\\tfn\\tln\\tfn_org\\tln_org\\n\") \n",
    "\n",
    "for i, l in tqdm(enumerate(fi), total=9928121 ):\n",
    "    qid, name, fn_org, ln_org, gender, desc, cc, dob, image, sort = l.strip('\\n').split('\\t')\n",
    "    rec = parse(name)\n",
    "    titles, fn, ln = [], [], []\n",
    "    prob = -1 \n",
    "    if rec['tags'] and rec['tags'][0]['prob']>0.2:\n",
    "        prob = rec['tags'][0]['prob']\n",
    "        for tok, typ in zip(rec['tags'][0]['tokens'], rec['tags'][0]['labels']):\n",
    "            if typ[:5] == 'title':\n",
    "                titles.append(tok)\n",
    "            elif typ[:2] == 'fn':\n",
    "                fn.append(tok)\n",
    "            elif typ[:2] == 'ln':\n",
    "                ln.append(tok)\n",
    "    fo.write(f\"{qid}\\t{name}\\t{prob}\\t{','.join(titles)}\\t{','.join(fn)}\\t{','.join(ln)}\\t{fn_org}\\t{ln_org}\\n\") \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2054c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /backup/wikidata/wiki_person.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk = 0\n",
    "fn = '/projekti/mondonomo/books/thai/thai_wiki.csv'\n",
    "fo = open(fn, 'w')\n",
    "fo2 = open(fn.replace('thai_', 'nonthai_'), 'w')\n",
    "for i, l in enumerate(open('/backup/wikidata/wiki_person.jsonl')):\n",
    "    j = json.loads(l)\n",
    "    cc = get_wiki_cc(j['country'], j['birth_place'], j['language'])\n",
    "    if 'en' in j['l'] and 'th' in j['l'] and cc:\n",
    "        native = j[\"name_native\"][0] if j[\"name_native\"] else ''\n",
    "        en = j[\"l\"][\"en\"][0].replace(',',' ')\n",
    "        th = j[\"l\"][\"th\"][0].replace(',',' ')\n",
    "        if cc == 'TH':\n",
    "            fo.write(f'{j[\"id\"]},{cc},{en},{th}\\n')\n",
    "        else:\n",
    "            fo2.write(f'{j[\"id\"]},{cc},{en},{th}\\n')\n",
    "        uk += 1\n",
    "fo.close()\n",
    "!head /projekti/mondonomo/books/thai/thai_wiki.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "uk = 0\n",
    "fn = '/backup/wikidata/hr_wiki.csv'\n",
    "fo = open(fn, 'w')\n",
    "fo2 = open(fn.replace('hr_', 'nonhr_'), 'w')\n",
    "for l in tqdm(open('/backup/wikidata/wiki_person.jsonl'), total=10_044_571):\n",
    "    j = json.loads(l)\n",
    "    cc = get_wiki_cc(j['country'], j['birth_place'], j['language'])\n",
    "    if 'en' in j['l'] and 'hr' in j['l'] and cc:\n",
    "        native = j[\"name_native\"][0] if j[\"name_native\"] else ''\n",
    "        en = j[\"l\"][\"en\"][0].replace(',',' ')\n",
    "        th = j[\"l\"][\"hr\"][0].replace(',',' ')\n",
    "        if cc == 'HR':\n",
    "            fo.write(f'{j[\"id\"]},{cc},{en},{th}\\n')\n",
    "        else:\n",
    "            fo2.write(f'{j[\"id\"]},{cc},{en},{th}\\n')\n",
    "        uk += 1\n",
    "fo.close()\n",
    "!head $fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5598f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('George Washington')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
